{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460f8316-600f-42ef-b89c-7e94a5b81de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import re\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import io\n",
    "\n",
    "def save_parquet_to_s3(df, bucket, key):\n",
    "    # Convert DataFrame to Parquet\n",
    "    table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "    parquet_buffer = io.BytesIO()\n",
    "    pq.write_table(table, parquet_buffer)\n",
    "\n",
    "    # Upload to S3\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.put_object(Bucket=bucket, Key=key, Body=parquet_buffer.getvalue())\n",
    "    print(f\"âœ… Parquet saved to s3://{bucket}/{key}\")\n",
    "    \n",
    "\n",
    "def read_log_from_s3(bucket, key):\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.get_object(Bucket=bucket, Key=key)\n",
    "    log_data = response['Body'].read().decode('utf-8')\n",
    "    return log_data\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # 1: read data from the bucket\n",
    "    # Get bucket and object key from the S3 event trigger\n",
    "    record = event['Records'][0]\n",
    "    bucket_name = record['s3']['bucket']['name']\n",
    "    input_key = record['s3']['object']['key']\n",
    "\n",
    "    print(f\"ðŸ“¥ Triggered by: s3://{bucket_name}/{input_key}\")\n",
    "\n",
    "    # Step 2: Read log data\n",
    "    raw_logs = read_log_from_s3(bucket_name, input_key)\n",
    "\n",
    "    # Split the log entries using the delimiter\n",
    "    entries = [entry.strip() for entry in raw_logs.split('---') if entry.strip()]\n",
    "\n",
    "    # Regex pattern to extract data\n",
    "    log_pattern = re.compile(\n",
    "        r'(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) \\[(?P<log_level>[A-Za-z0-9_]+)\\] '\n",
    "        r'(?P<component>[^\\s]+) - TicketID=(?P<ticket_id>[^\\s]+) SessionID=(?P<session_id>[^\\s]+)\\s*'\n",
    "        r'IP=(?P<ip>.*?) \\| ResponseTime=(?P<response_time>-?\\d+)ms \\| CPU=(?P<cpu>[\\d.]+)% \\| EventType=(?P<event_type>.*?) \\| Error=(?P<error>\\w+)\\s*'\n",
    "        r'UserAgent=\"(?P<user_agent>.*?)\"\\s*'\n",
    "        r'Message=\"(?P<message>.*?)\"\\s*'\n",
    "        r'Debug=\"(?P<debug>.*?)\"\\s*'\n",
    "        r'TraceID=(?P<trace_id>.*)'\n",
    "    )\n",
    "\n",
    "    # Extract structured data\n",
    "    parsed_entries = []\n",
    "    for entry in entries:\n",
    "        match = log_pattern.search(entry)\n",
    "        if match:\n",
    "            parsed_entries.append(match.groupdict())\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(parsed_entries)\n",
    "\n",
    "    # Data cleaning\n",
    "    # i) Drop trace_id column\n",
    "    df = df.drop('trace_id', axis=1)\n",
    "\n",
    "    # ii) Remove Negative response time\n",
    "    df = df[df['response_time'].astype(int) >= 0]\n",
    "\n",
    "    # iii) typo-fix in log_level \n",
    "    fix_log_level = {'INF0': 'INFO', 'DEBG': 'DEBUG', 'warnING': 'WARNING', 'EROR': 'ERROR'}\n",
    "    df['log_level'] = df['log_level'].replace(fix_log_level)\n",
    "\n",
    "    # iv) Remove duplicate rows\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # v) Change to appropriate data types (response_time, cpu, timestamp, error)\n",
    "    df['response_time'] = df['response_time'].astype(int)\n",
    "    df['cpu'] = df['cpu'].astype(float)\n",
    "    df['error'] = df['error'].str.lower().map({'true': True, 'false': False})\n",
    "\n",
    "    # timestamp conversion\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S', errors='coerce').astype('datetime64[ms]')\n",
    "    print(df.shape)\n",
    "    print(df.head())\n",
    "\n",
    "    # Save the data (Upload Parquet to S3)\n",
    "    output_file_name = input_key.split('/')[2].replace('.log', '.parquet')\n",
    "    output_key = f'support-logs/processed/{output_file_name}'\n",
    "    save_parquet_to_s3(df, bucket_name, output_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
